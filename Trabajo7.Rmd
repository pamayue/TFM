---
title: "Trabajo7"
author: "Pablo Mas Cayuelas"
date: "13/12/2020"
output: html_document
---

OBJETIVO: VER SI HAY POLARIZACIÓN O NO


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rtweet)
library(twitteR)
library(dplyr)
library(rlang)
library(tidyr)
library(rio)
```

```{r}
library(bitops)
library(RCurl)
library(rjson)
library(twitteR)
library(tm)
library(Rcpp)
library(RColorBrewer)
library(wordcloud)
library(tidytext)
library(purrr)
library(stringr)
```

```{r}
sentimientos <- get_sentiments(lexicon = "bing")
sentimientos <- sentimientos %>%
               mutate(valor = if_else(sentiment == "negative", -1, 1))
```

```{r}
limpiar_tokenizar <- function(texto){
  # El orden de la limpieza no es arbitrario
    # Se convierte todo el texto a minúsculas
    nuevo_texto <- tolower(texto)
    # Eliminación de páginas web (palabras que empiezan por "http." seguidas 
    # de cualquier cosa que no sea un espacio)
    nuevo_texto <- str_replace_all(nuevo_texto,"http\\S*", "")
    # Eliminación de signos de puntuación
    nuevo_texto <- str_replace_all(nuevo_texto,"[[:punct:]]", " ")
    # Eliminación de números
    nuevo_texto <- str_replace_all(nuevo_texto,"[[:digit:]]", " ")
    # Eliminación de espacios en blanco múltiples
    nuevo_texto <- str_replace_all(nuevo_texto,"[\\s]+", " ")
    # Tokenización por palabras individuales
    nuevo_texto <- str_split(nuevo_texto, " ")[[1]]
    # Eliminación de tokens con una longitud < 2
    nuevo_texto <- keep(.x = nuevo_texto, .p = function(x){str_length(x) > 1})
    return(nuevo_texto)
}
```


```{r}
# Se aplica la función de limpieza y tokenización a cada tweet
populartuits2 <- populartuits %>% mutate(texto_tokenizado = map(.x = text,
                                                   .f = limpiar_tokenizar))
populartuits2 %>% slice(1) %>% select(texto_tokenizado) %>% pull()

populartuits_tidy <- populartuits2 %>% select(-text) %>% unnest()
populartuits_tidy <- populartuits_tidy %>% rename(token = texto_tokenizado)

lista_stopwords <- c('me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves',
               'you','your', 'yours', 'yourself', 'yourselves', 'he', 'him','his',
               'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',
               'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',
               'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',
               'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',
               'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and',
               'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at',
               'by', 'for', 'with', 'about', 'against', 'between', 'into',
               'through', 'during', 'before', 'after', 'above', 'below', 'to',
               'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under',
               'again', 'further', 'then', 'once', 'here', 'there', 'when',
               'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',
               'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own',
               'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will',
               'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've',
               'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn',
               'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan',
               'shouldn', 'wasn', 'weren', 'won', 'wouldn','i')
# Se añade el término amp al listado de stopwords
lista_stopwords <- c(lista_stopwords, "amp")

# Se filtran las stopwords
populartuits_tidy <- populartuits_tidy %>% filter(!(token %in% lista_stopwords))

populartuits_tidy %>% group_by(token) %>% count(token) %>%
            top_n(10, n) %>% arrange(desc(n)) %>% print(n=30)
```


```{r}
populartuits_sent <- inner_join(x = populartuits_tidy, y = sentimientos,
                          by = c("token" = "word"))
```

```{r}
positivos <- populartuits_sent[populartuits_sent$sentiment=="positive", ]
negativos <- populartuits_sent[populartuits_sent$sentiment=="negative", ]
neutrales <- populartuits_sent[populartuits_sent$sentiment=="neutral", ]
```

```{r}
barplot(table(populartuits_sent$sentiment), # datos género
        main="Distribución sentimientos de los tuits populares", # título
        col=rgb(0.8,0.1,0.1,0.6), # damos color a las barras
        ylab="Frecuencia") # etiqueta de las ordenadas

length(populartuits_sent$token)/length(populartuits_tidy$token)*100
```


```{r}
# Se aplica la función de limpieza y tokenización a cada tweet
trump_token <- trump %>% mutate(texto_tokenizado = map(.x = text,
                                                   .f = limpiar_tokenizar))
trump_token %>% slice(1) %>% select(texto_tokenizado) %>% pull()

trump_token_tidy <- trump_token %>% select(-text) %>% unnest()
trump_token_tidy <- trump_token_tidy %>% rename(token = texto_tokenizado)

head(trump_token_tidy)
```


```{r}
trump_token_sent <- inner_join(x = trump_token_tidy, y = sentimientos,
                          by = c("token" = "word"))
```

```{r}
barplot(table(trump_token_sent$sentiment), # datos género
        main="Distribución sentimientos de los tuits que contienen trump", # título
        col=rgb(0.8,0.1,0.1,0.6), # damos color a las barras
        ylab="Frecuencia") # etiqueta de las ordenadas

length(trump_token_sent$token)/length(trump_token_tidy$token)*100
```

```{r}
# Se aplica la función de limpieza y tokenización a cada tweet
atrump_token <- atrump %>% mutate(texto_tokenizado = map(.x = text,
                                                   .f = limpiar_tokenizar))
atrump_token %>% slice(1) %>% select(texto_tokenizado) %>% pull()

atrump_token_tidy <- atrump_token %>% select(-text) %>% unnest()
atrump_token_tidy <- atrump_token_tidy %>% rename(token = texto_tokenizado)

head(atrump_token_tidy)
```


```{r}
atrump_token_sent <- inner_join(x = atrump_token_tidy, y = sentimientos,
                          by = c("token" = "word"))
```

```{r}
barplot(table(atrump_token_sent$sentiment), # datos género
        main="Distribución sentimientos de los tuits que mencionan a Trump", # título
        col=rgb(0.8,0.1,0.1,0.6), # damos color a las barras
        ylab="Frecuencia") # etiqueta de las ordenadas
```

```{r}
length(atrump_token_sent$token)/length(atrump_token_tidy$token)*100
```


```{r}
# Se aplica la función de limpieza y tokenización a cada tweet
biden_token <- biden %>% mutate(texto_tokenizado = map(.x = text,
                                                   .f = limpiar_tokenizar))
biden_token %>% slice(1) %>% select(texto_tokenizado) %>% pull()

biden_token_tidy <- biden_token %>% select(-text) %>% unnest()
biden_token_tidy <- biden_token_tidy %>% rename(token = texto_tokenizado)

head(biden_token_tidy)
```


```{r}
biden_token_sent <- inner_join(x = biden_token_tidy, y = sentimientos,
                          by = c("token" = "word"))
```

```{r}
barplot(table(biden_token_sent$sentiment), # datos género
        main="Distribución sentimientos de los tuits que contienen biden", # título
        col=rgb(0.8,0.1,0.1,0.6), # damos color a las barras
        ylab="Frecuencia") # etiqueta de las ordenadas

length(biden_token_sent$token)/length(biden_token_tidy$token)*100
```

```{r}
# Se aplica la función de limpieza y tokenización a cada tweet
abiden_token <- abiden %>% mutate(texto_tokenizado = map(.x = text,
                                                   .f = limpiar_tokenizar))
abiden_token %>% slice(1) %>% select(texto_tokenizado) %>% pull()

abiden_token_tidy <- abiden_token %>% select(-text) %>% unnest()
abiden_token_tidy <- abiden_token_tidy %>% rename(token = texto_tokenizado)

head(abiden_token_tidy)
```


```{r}
abiden_token_sent <- inner_join(x = abiden_token_tidy, y = sentimientos,
                          by = c("token" = "word"))
```

```{r}
barplot(table(abiden_token_sent$sentiment), # datos género
        main="Distribución sentimientos de los tuits que mencionan a Biden", # título
        col=rgb(0.8,0.1,0.1,0.6), # damos color a las barras
        ylab="Frecuencia") # etiqueta de las ordenadas

length(abiden_token_sent$token)/length(abiden_token_tidy$token)*100
```
```{r}
# Se aplica la función de limpieza y tokenización a cada tweet
MAGA_token <- MAGA %>% mutate(texto_tokenizado = map(.x = text,
                                                   .f = limpiar_tokenizar))
MAGA_token %>% slice(1) %>% select(texto_tokenizado) %>% pull()

MAGA_token_tidy <- MAGA_token %>% select(-text) %>% unnest()
MAGA_token_tidy <- MAGA_token_tidy %>% rename(token = texto_tokenizado)

head(MAGA_token_tidy)
```


```{r}
MAGA_token_sent <- inner_join(x = MAGA_token_tidy, y = sentimientos,
                          by = c("token" = "word"))
```

```{r}
barplot(table(MAGA_token_sent$sentiment), # datos género
        main="Distribución sentimientos de los tuits que contienen MAGA", # título
        col=rgb(0.8,0.1,0.1,0.6), # damos color a las barras
        ylab="Frecuencia") # etiqueta de las ordenadas

length(MAGA_token_sent$token)/length(MAGA_token_tidy$token)*100
```

```{r}
# Se aplica la función de limpieza y tokenización a cada tweet
Trump2020_token <- Trump2020 %>% mutate(texto_tokenizado = map(.x = text,
                                                   .f = limpiar_tokenizar))
Trump2020_token %>% slice(1) %>% select(texto_tokenizado) %>% pull()

Trump2020_token_tidy <- Trump2020_token %>% select(-text) %>% unnest()
Trump2020_token_tidy <- Trump2020_token_tidy %>% rename(token = texto_tokenizado)
```


```{r}
Trump2020_token_sent <- inner_join(x = Trump2020_token_tidy, y = sentimientos,
                          by = c("token" = "word"))
```

```{r}
barplot(table(Trump2020_token_sent$sentiment), # datos género
        main="Distribución sentimientos de los tuits que contienen Trump2020", # título
        col=rgb(0.8,0.1,0.1,0.6), # damos color a las barras
        ylab="Frecuencia") # etiqueta de las ordenadas

length(Trump2020_token_sent$token)/length(Trump2020_token_tidy$token)*100
```